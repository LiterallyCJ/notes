
## Symmetric Matrices

$\vec x_0 = (\frac{1}{n}, \frac{1}{n}, \dots, \frac{1}{n}) \in \mathbb{R}^n$ and calculate $A \vec x_0, A^2 \vec x_0, A^3 \vec x_0, ect$.
As $k \to \infty$, the vectors $A^k$ converge to some vector $\vec q \in \mathbb{R}^n$ such that $A \vec q = \vec q$ and the sum of coordinates in $\vec q$ is 1.

A square matrix $A$ is symmetric if $A^T = A$.

Let $A \in M_n(\mathbb{R})$ be an $n \times n$ symmetric matrix. Then $A$ has $n$ linearly independent eigenvectors (if they were put into a matrix, it will have a determinant i.e they are not multiples of each other) and all eigenvalues of $A$ are real numbers.

If $\lambda$ is an eigenvalue of an $n \times n$ matrix $A$, then the number of linearly independent eigenvectors of $A$ corresponding to $\lambda$ is equal to the number of free variables in the solution of the system $(A - \lambda I _n) \vec v = \vec 0$

