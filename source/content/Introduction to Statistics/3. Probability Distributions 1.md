date

## Random variables

For discrete variables, the probability function is the probability mass function (PMF), for continuous functions it is the probability density function (PDF).

A random variable maps each outcome in the sample space to a real number. For example consider a fair coin toss with outcomes head or tail. A random variable that maps head to 1 and tail to 0 looks like:
$$
Head \to 1,\ Tail \to 0
$$
We can denote this random variable as $X$ which is the number of heads obtained by tossing a single coin. The possible values for $X$ are 0 and 1.

## Discrete and continuous random variables.

A random variable is discrete if it takes a finite or countably infinite set of values (e.g. the number of apple users among 20 students; the number of credit cards a person carries). 
It is continuous if it can take any real value. Some variables mix discrete and continuous parts e.g. daily rainfall (0 on some days, or a positive continuous amount on other days)

## Probability distribution of a random variable

By the first axiom of probability, the total probability is 1. Because a random variable maps outcomes to the real line, the probabilities assigned to all its possible values must sum or integrate to 1. 
A probability distribution allocates this total probability across those values.

In the coin tossing experiment, lets assume that the chances of landing on heads is $p$, therefore since there are only 2 outcomes, the probability of landing on tails is $1 - p$. This means that the probability that $Y = 1$ is $p$, and the probability $Y = 0$ is $1 - p$. Formally, this is written as:
$$
P(Y = y) = 
\begin{cases}
1 - p & \text{for } y = 0 \\
p & \text{for } y = 1
\end{cases}
$$
This is called the Bernoulli distribution with parameter $p$, the simplest discrete distribution.

For a discrete random variable, we define function $f(x)$ to denote $P(X = x)$ or $f(y)$ to denote $P(Y = y)$ and call the function $f(x)$ the probability mass function (pmf) of X. Not every function qualifies, as it requires:
- $f(x) \geq 0$ for all possible values of $x$
- $\sum_{\text{all } x}f()x = 1$

## Continuous random variables

For a continuous random variable $X$, probabilities derive from a nonnegative function $f(x$), the probability density function (pdf). We compute probabilities via integrals e.g.
$$
P(a < X < b) = \int^b_a f(u)\ du
$$
This can be interpreted as the area under the curve $f(x)$ in the interval $(a, b)$. We do not use $f(x) = P(X = x)$ for any $x$, because we set $P(X = x) = 0$ by convention.

Since we are dealing with probabilities which are always between 0 and 1, we cannot take any function $f(x)$ as a pdf for a random variable. For $f(x)$ to be a pdf, we must have:
- $f(x) \geq 0$ for all possible values of $x$, i.e. $-\infty < x < \infty$
- $\int^\infty_{-\infty}f(u)\ du = 1$

## Cumulative distribution function

We may also use the cumulative distribution function (cdf) which gives the probability that the random variable is less than or equal to a given value.

For a discrete random variable $X$, the cdf is the cumulative sum of the pmf $f(u)$ up to and including $u = x$ which formally is,
$$
P(X \leq x) = F(x) = \sum_{u \leq x}f(u)
$$
The cdf for a discrete random variable is a step function. The jump-points are the possible values of the random variable, and the height of a jump gives the probability of the random variable taking that value. It is clear that the pmf is uniquely determined by the cdf.

For a continuous random variable $X$, the cdf is defined as,
$$
P(X \leq x) \equiv F(x) = \int^x_{- \infty}f(u)\ du
$$
From the fundamental theorem of calculus, we know that
$$
f(x) = \frac{dF(x)}{dx}
$$
so for a continuous random variable, the pdf is the derivative of the cdf.


