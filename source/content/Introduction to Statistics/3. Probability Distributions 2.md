21/10/25

# Summaries of a random variable

A random variable $X$ with either a pmf $f(x)$ or a pdf $f(x)$ can be summarised similar to means and variance of  data samples.

## Expectation

The mean of $X$ or the expectation is defined as:

$$
E(x) = 
\begin{cases}
\sum_{\text{all } x} x \cdot f(x) & \text{if } X \text{ is discrete} \\
\int^\infty_{-\infty} x \cdot f(x)\ dx &\text{if } X \text{ is continuous}
\end{cases}
$$
When the sum or the integral exists. Informally, you can describe the expectation as the sum or integral of "value $\times$ probability".

This means we just get the value multiplied by the probability for every value possible.

You can think of the expectation as a weighted average (mean) of a random variable i.e. the expected value.

ex.
In a fair die, with each of the six sides having a probability of $\frac{1}{6}$ of landing face up, let $X$ be the number on the up-face of the die, then
$$
\begin{align}
E(X) 
&= \sum^6_{x = 1} x \cdot P(X = x) \\
&= \sum^6_{x = 1}\frac{x}{6} \\
&= 3.5
\end{align}
$$

ex 2.
Suppose $X~U(a, b)$, with pdf $f(x) = \frac{1}{b-a}$, $a < x < b$.
Then,
$$
\begin{align}
E(X)
&= \int^\infty_{-\infty} x \cdot f(x)\ dx \\
&= \int^b_a \frac{x}{b-a}\ dx \\
& = \frac{b^2 - a^2}{2(b-a)} \\
&= \frac{b+a}{2}
\end{align}
$$
for the midpoint of the interval $(a, b)$

If $Y = g(X)$ for any function $g(\cdot)$, then $Y$ is a random variable as well. To find $E(Y)$, we simply use the value times probability rule, i.e. the expected value of $Y$ is either sum or integral of its value $g(x$) times probability $f(x)$:
$$
E(Y) = E(g(X)) =
\begin{cases}
\sum_{\text{all } x} g(x) \cdot f(x) &\text{if } X \text{ is discrete} \\
\int^\infty_{-\infty}g(x) \cdot f(x)\ dx &\text{if } X \text{ is continuous}
\end{cases}
$$

Note that this is the same as earlier, but $x$ was replaced with $g(x)$ to get the new value.

For example if $X$ is continuous, then $E(X^2) = \int^\infty_{-\infty} x^2 \cdot f(x)\ dx$.

## Linearity of expectation

If $Y = 






