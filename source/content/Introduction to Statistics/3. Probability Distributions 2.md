21/10/25

# Summaries of a random variable

A random variable $X$ with either a pmf $f(x)$ or a pdf $f(x)$ can be summarised similar to means and variance of  data samples.

## Expectation

The mean of $X$ or the expectation is defined as:

$$
E(x) = 
\begin{cases}
\sum_{\text{all } x} x \cdot f(x) & \text{if } X \text{ is discrete} \\
\int^\infty_{-\infty} x \cdot f(x)\ dx &\text{if } X \text{ is continuous}
\end{cases}
$$
When the sum or the integral exists. Informally, you can describe the expectation as the sum or integral of "value $\times$ probability".

You can think of the expectation as a weighted average (mean) of a 

ex.
In a fair die, with each of the six sides having a probability of $\frac{1}{6}$ of landing face up, let $X$ be the number on the up-face of the die, then
$$
\begin{align}
E(X) 
&= \sum^6_{x = 1} x \cdot P(X = x) \\
&= \sum^6_{x = 1}\frac{x}{6} \\
&= 3.5
\end{align}
$$

ex 2.
Suppose $X~U(a, b)$, with pdf $f(x) = \frac{1}{b-a}$, $a < x < b$.
Then,
$$
\begin{align}
E(X)
&= \int^\infty_{-\infty} x \cdot f(x)\ dx \\
&= \int^b_a \frac{x}{b-a}\ dx \\
& = \frac{b^2 - a^2}{2(b-a)} \\
&= \frac{b+a}{2}
\end{align}
$$
for the midpoint of the interval $(a, b)$





