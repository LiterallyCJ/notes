21/10/25

# Summaries of a random variable

A random variable $X$ with either a pmf $f(x)$ or a pdf $f(x)$ can be summarised similar to means and variance of  data samples.

## Expectation

The mean of $X$ or the expectation is defined as:

$$
E(x) = 
\begin{cases}
\sum_{\text{all } x} x \cdot f(x) & \text{if } X \text{ is discrete} \\
\int^\infty_{-\infty} x \cdot f(x)\ dx &\text{if } X \text{ is continuous}
\end{cases}
$$
When the sum or the integral exists. Informally, you can describe the expectation as the sum or integral of "value $\times$ probability".

This means we just get the value multiplied by the probability for every value possible. Remember that f(x) is the pmf or pdf of the random variable.

You can think of the expectation as a weighted average (mean) of a random variable i.e. the expected value.

ex.
In a fair die, with each of the six sides having a probability of $\frac{1}{6}$ of landing face up, let $X$ be the number on the up-face of the die, then
$$
\begin{align}
E(X) 
&= \sum^6_{x = 1} x \cdot P(X = x) \\
&= \sum^6_{x = 1}\frac{x}{6} \\
&= 3.5
\end{align}
$$

ex 2.
Suppose $X~U(a, b)$, with pdf $f(x) = \frac{1}{b-a}$, $a < x < b$.
Then,
$$
\begin{align}
E(X)
&= \int^\infty_{-\infty} x \cdot f(x)\ dx \\
&= \int^b_a \frac{x}{b-a}\ dx \\
& = \frac{b^2 - a^2}{2(b-a)} \\
&= \frac{b+a}{2}
\end{align}
$$
for the midpoint of the interval $(a, b)$

If $Y = g(X)$ for any function $g(\cdot)$, then $Y$ is a random variable as well. To find $E(Y)$, we simply use the value times probability rule, i.e. the expected value of $Y$ is either sum or integral of its value $g(x$) times probability $f(x)$:
$$
E(Y) = E(g(X)) =
\begin{cases}
\sum_{\text{all } x} g(x) \cdot f(x) &\text{if } X \text{ is discrete} \\
\int^\infty_{-\infty}g(x) \cdot f(x)\ dx &\text{if } X \text{ is continuous}
\end{cases}
$$

Note that this is the same as earlier, but $x$ was replaced with $g(x)$ to get the new value.

For example if $X$ is continuous, then $E(X^2) = \int^\infty_{-\infty} x^2 \cdot f(x)\ dx$.

### Linearity of expectation

If $Y = aX + b$, then, $E(Y) = a \cdot E(X) + b$

<u>Proof:</u>

$$
\begin{align}
E(Y) 
&= \int^\infty_{-\infty} (ax + b) \cdot f(x)\ dx \\
&= a \int^\infty_{-\infty}xf(x)\ dx + b \int^\infty_{-\infty}f(x)\ dx \\
&= aE(X) + b
\end{align}
$$
This also works with discrete variables, just change the integral for summation.

Note:
We used the fact that $\int^\infty_{-\infty}f(x)\ dx = 1$. 

Suppose we know $E(X) = 5$ and $Y = -2X + 549$, we know $E(Y) = 539$

If the pmf/pdf of $X$ is symmetric about $c$ (i.e $f(c + x) = f(c - x)$ for all $x > 0$), then $E(X) = c$

## Variance

Variance measures the spread of a random variable and is defined by:
$$
Var(X) = E(X - \mu)^2 = E(X^2) - \mu^2
$$
Where $\mu$ is the mean of $X$

We can also get variance by:
$$
Var(X) = E(X - \mu)^2 = 
\begin{cases}
\sum_{\text{all } x} (x - \mu)^2f(x) &\text{if } X \text{ is discrete} \\
\int^\infty_{-\infty}(x-\mu)^2f(x)\ dx &\text{if } X \text{ is continuous}
\end{cases}
$$

<u>Proof:</u>

$$
\begin{align}
Var(X)
&= E(X - \mu)^2 \\
&= E(X^2 - 2X\mu + \mu^2) \\
&= E(X^2) - 2\mu E(X) + \mu^2 \\
&= E(X^2) - 2\mu \mu + \mu^2 \\
&= E(X^2) - \mu^2
\end{align}
$$
Therefore the variance is equal to the expected square minus the square of the mean. We denote variance by $\sigma^2$ which is always non-negative and equals zero only if $X = \mu$ with probability 1 (not random). The square root, $\sigma$ is the standard deviation.

ex.

Suppose $X~U(a,b)$ with pdf $f(x) = \frac{1}{b-a}, a < x < b$, then
$$
\begin{align}
E(X^2) 
&= \int^b_a \frac{x^2}{b-a}\ dx \\\\
&= \frac{b^3 - a^3}{3(b-a)} \\\\
&= \frac{b^2 + ab + a^2}{3}
\end{align}
$$
Hence,
$$
Var(X) = \frac{b^2 + ab + a^2}{3} -(\frac{b+a}{2})^2 = \frac{(b-a)^2}{12}
$$